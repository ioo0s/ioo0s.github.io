<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

    
    <title>Reinforcement Learning Note | ioo0s&#39;s blog</title>

    <meta name="description" content="&lt;h2 id=&#34;Q-Learning&#34;&gt;&lt;a href=&#34;#Q-Learning&#34; class=&#34;headerlink&#34; title=&#34;Q-Learning&#34;&gt;&lt;/a&gt;Q-Learning&lt;/h2&gt;&lt;h3 id=&#34;åŸç†&#34;&gt;&lt;a href=&#34;#åŸç†&#34; class=&#34;headerlink&#34; title=&#34;åŸç†&#34;&gt;&lt;/a&gt;åŸç†&lt;/h3&gt;&lt;p&gt;è¯¾ç¨‹å‚è€ƒï¼š&lt;a href=&#34;https://www.bilibili.com/video/BV13W411Y75P&#34;&gt;https://www.bilibili.com/video/BV13W411Y75P&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Q-Learningæ˜¯å±äºå€¼å‡½æ•°è¿‘ä¼¼ç®—æ³•ä¸­ï¼Œè’™ç‰¹å¡æ´›æ–¹æ³•å’Œæ—¶é—´å·®åˆ†æ³•ç›¸ç»“åˆçš„ç®—æ³•ã€‚è¿™ç§ç®—æ³•ä½¿å¾—æ™ºèƒ½ä½“ï¼ˆagentï¼‰èƒ½å¤Ÿåœ¨ä¸ç¯å¢ƒäº’åŠ¨çš„è¿‡ç¨‹ä¸­å­¦ä¹ å¦‚ä½•é‡‡å–åŠ¨ä½œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚Q-learningç‰¹åˆ«é€‚ç”¨äºè§£å†³å†³ç­–è¿‡ç¨‹é—®é¢˜ï¼Œå°¤å…¶æ˜¯é‚£äº›çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å®šä¹‰æ˜ç¡®çš„é—®é¢˜ã€‚&lt;/p&gt;
&lt;p&gt;Q-Learning æ˜¯ä¸€ä¸ªç¦»çº¿ç­–ç•¥ï¼ˆoff-policyï¼‰å­¦ä¹ ç®—æ³•ã€‚åœ¨Q-Learningä¸­ï¼Œæ™ºèƒ½ä½“å­¦ä¹ çš„æ˜¯ä¸€ä¸ªä¸å…¶å®é™…æ‰§è¡ŒåŠ¨ä½œæ— å…³çš„ä¼˜åŒ–ç­–ç•¥ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“å®ƒåœ¨æ¢ç´¢æ›´å¤šçš„çŠ¶æ€-åŠ¨ä½œå¯¹æ—¶ï¼Œå®ƒå­¦ä¹ çš„æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚åŒæ—¶ï¼Œåœ¨æ›´æ–°q-tableä¸­çš„å€¼æ—¶ï¼Œå¹¶ä¸è€ƒè™‘ä¸‹ä¸€æ­¥å®é™…æ‰§è¡Œçš„åŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Œè€Œæ˜¯å‡è®¾é‡‡å–çš„æ˜¯è®©next_stateä¸‹q-tableå€¼æœ€å¤§çš„åŠ¨ä½œã€‚&lt;/p&gt;">
    <meta name="keywords" content="">

    

    <meta property="og:locale" content="zh" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content= "Reinforcement Learning Note | ioo0s&#39;s blog"  />
    <meta property="og:description" content= "&lt;h2 id=&#34;Q-Learning&#34;&gt;&lt;a href=&#34;#Q-Learning&#34; class=&#34;headerlink&#34; title=&#34;Q-Learning&#34;&gt;&lt;/a&gt;Q-Learning&lt;/h2&gt;&lt;h3 id=&#34;åŸç†&#34;&gt;&lt;a href=&#34;#åŸç†&#34; class=&#34;headerlink&#34; title=&#34;åŸç†&#34;&gt;&lt;/a&gt;åŸç†&lt;/h3&gt;&lt;p&gt;è¯¾ç¨‹å‚è€ƒï¼š&lt;a href=&#34;https://www.bilibili.com/video/BV13W411Y75P&#34;&gt;https://www.bilibili.com/video/BV13W411Y75P&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Q-Learningæ˜¯å±äºå€¼å‡½æ•°è¿‘ä¼¼ç®—æ³•ä¸­ï¼Œè’™ç‰¹å¡æ´›æ–¹æ³•å’Œæ—¶é—´å·®åˆ†æ³•ç›¸ç»“åˆçš„ç®—æ³•ã€‚è¿™ç§ç®—æ³•ä½¿å¾—æ™ºèƒ½ä½“ï¼ˆagentï¼‰èƒ½å¤Ÿåœ¨ä¸ç¯å¢ƒäº’åŠ¨çš„è¿‡ç¨‹ä¸­å­¦ä¹ å¦‚ä½•é‡‡å–åŠ¨ä½œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚Q-learningç‰¹åˆ«é€‚ç”¨äºè§£å†³å†³ç­–è¿‡ç¨‹é—®é¢˜ï¼Œå°¤å…¶æ˜¯é‚£äº›çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å®šä¹‰æ˜ç¡®çš„é—®é¢˜ã€‚&lt;/p&gt;
&lt;p&gt;Q-Learning æ˜¯ä¸€ä¸ªç¦»çº¿ç­–ç•¥ï¼ˆoff-policyï¼‰å­¦ä¹ ç®—æ³•ã€‚åœ¨Q-Learningä¸­ï¼Œæ™ºèƒ½ä½“å­¦ä¹ çš„æ˜¯ä¸€ä¸ªä¸å…¶å®é™…æ‰§è¡ŒåŠ¨ä½œæ— å…³çš„ä¼˜åŒ–ç­–ç•¥ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“å®ƒåœ¨æ¢ç´¢æ›´å¤šçš„çŠ¶æ€-åŠ¨ä½œå¯¹æ—¶ï¼Œå®ƒå­¦ä¹ çš„æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚åŒæ—¶ï¼Œåœ¨æ›´æ–°q-tableä¸­çš„å€¼æ—¶ï¼Œå¹¶ä¸è€ƒè™‘ä¸‹ä¸€æ­¥å®é™…æ‰§è¡Œçš„åŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Œè€Œæ˜¯å‡è®¾é‡‡å–çš„æ˜¯è®©next_stateä¸‹q-tableå€¼æœ€å¤§çš„åŠ¨ä½œã€‚&lt;/p&gt;" />
    <meta property="og:url" content="http://ioo0s.art/2024/04/10/Reinforcement-Learning-Note/index.html" />
    <meta property="og:site_name" content="" />
    <meta property="article:author" content="ios" />
    <meta property="article:publisher" content="" />
    <meta property="og:description" content="&lt;h2 id=&#34;Q-Learning&#34;&gt;&lt;a href=&#34;#Q-Learning&#34; class=&#34;headerlink&#34; title=&#34;Q-Learning&#34;&gt;&lt;/a&gt;Q-Learning&lt;/h2&gt;&lt;h3 id=&#34;åŸç†&#34;&gt;&lt;a href=&#34;#åŸç†&#34; class=&#34;headerlink&#34; title=&#34;åŸç†&#34;&gt;&lt;/a&gt;åŸç†&lt;/h3&gt;&lt;p&gt;è¯¾ç¨‹å‚è€ƒï¼š&lt;a href=&#34;https://www.bilibili.com/video/BV13W411Y75P&#34;&gt;https://www.bilibili.com/video/BV13W411Y75P&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Q-Learningæ˜¯å±äºå€¼å‡½æ•°è¿‘ä¼¼ç®—æ³•ä¸­ï¼Œè’™ç‰¹å¡æ´›æ–¹æ³•å’Œæ—¶é—´å·®åˆ†æ³•ç›¸ç»“åˆçš„ç®—æ³•ã€‚è¿™ç§ç®—æ³•ä½¿å¾—æ™ºèƒ½ä½“ï¼ˆagentï¼‰èƒ½å¤Ÿåœ¨ä¸ç¯å¢ƒäº’åŠ¨çš„è¿‡ç¨‹ä¸­å­¦ä¹ å¦‚ä½•é‡‡å–åŠ¨ä½œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚Q-learningç‰¹åˆ«é€‚ç”¨äºè§£å†³å†³ç­–è¿‡ç¨‹é—®é¢˜ï¼Œå°¤å…¶æ˜¯é‚£äº›çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å®šä¹‰æ˜ç¡®çš„é—®é¢˜ã€‚&lt;/p&gt;
&lt;p&gt;Q-Learning æ˜¯ä¸€ä¸ªç¦»çº¿ç­–ç•¥ï¼ˆoff-policyï¼‰å­¦ä¹ ç®—æ³•ã€‚åœ¨Q-Learningä¸­ï¼Œæ™ºèƒ½ä½“å­¦ä¹ çš„æ˜¯ä¸€ä¸ªä¸å…¶å®é™…æ‰§è¡ŒåŠ¨ä½œæ— å…³çš„ä¼˜åŒ–ç­–ç•¥ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“å®ƒåœ¨æ¢ç´¢æ›´å¤šçš„çŠ¶æ€-åŠ¨ä½œå¯¹æ—¶ï¼Œå®ƒå­¦ä¹ çš„æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚åŒæ—¶ï¼Œåœ¨æ›´æ–°q-tableä¸­çš„å€¼æ—¶ï¼Œå¹¶ä¸è€ƒè™‘ä¸‹ä¸€æ­¥å®é™…æ‰§è¡Œçš„åŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Œè€Œæ˜¯å‡è®¾é‡‡å–çš„æ˜¯è®©next_stateä¸‹q-tableå€¼æœ€å¤§çš„åŠ¨ä½œã€‚&lt;/p&gt;" />
    <meta name="twitter:title" content="Reinforcement Learning Note | ioo0s&#39;s blog"/>
    <meta name="twitter:description" content="&lt;h2 id=&#34;Q-Learning&#34;&gt;&lt;a href=&#34;#Q-Learning&#34; class=&#34;headerlink&#34; title=&#34;Q-Learning&#34;&gt;&lt;/a&gt;Q-Learning&lt;/h2&gt;&lt;h3 id=&#34;åŸç†&#34;&gt;&lt;a href=&#34;#åŸç†&#34; class=&#34;headerlink&#34; title=&#34;åŸç†&#34;&gt;&lt;/a&gt;åŸç†&lt;/h3&gt;&lt;p&gt;è¯¾ç¨‹å‚è€ƒï¼š&lt;a href=&#34;https://www.bilibili.com/video/BV13W411Y75P&#34;&gt;https://www.bilibili.com/video/BV13W411Y75P&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Q-Learningæ˜¯å±äºå€¼å‡½æ•°è¿‘ä¼¼ç®—æ³•ä¸­ï¼Œè’™ç‰¹å¡æ´›æ–¹æ³•å’Œæ—¶é—´å·®åˆ†æ³•ç›¸ç»“åˆçš„ç®—æ³•ã€‚è¿™ç§ç®—æ³•ä½¿å¾—æ™ºèƒ½ä½“ï¼ˆagentï¼‰èƒ½å¤Ÿåœ¨ä¸ç¯å¢ƒäº’åŠ¨çš„è¿‡ç¨‹ä¸­å­¦ä¹ å¦‚ä½•é‡‡å–åŠ¨ä½œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚Q-learningç‰¹åˆ«é€‚ç”¨äºè§£å†³å†³ç­–è¿‡ç¨‹é—®é¢˜ï¼Œå°¤å…¶æ˜¯é‚£äº›çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å®šä¹‰æ˜ç¡®çš„é—®é¢˜ã€‚&lt;/p&gt;
&lt;p&gt;Q-Learning æ˜¯ä¸€ä¸ªç¦»çº¿ç­–ç•¥ï¼ˆoff-policyï¼‰å­¦ä¹ ç®—æ³•ã€‚åœ¨Q-Learningä¸­ï¼Œæ™ºèƒ½ä½“å­¦ä¹ çš„æ˜¯ä¸€ä¸ªä¸å…¶å®é™…æ‰§è¡ŒåŠ¨ä½œæ— å…³çš„ä¼˜åŒ–ç­–ç•¥ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“å®ƒåœ¨æ¢ç´¢æ›´å¤šçš„çŠ¶æ€-åŠ¨ä½œå¯¹æ—¶ï¼Œå®ƒå­¦ä¹ çš„æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚åŒæ—¶ï¼Œåœ¨æ›´æ–°q-tableä¸­çš„å€¼æ—¶ï¼Œå¹¶ä¸è€ƒè™‘ä¸‹ä¸€æ­¥å®é™…æ‰§è¡Œçš„åŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Œè€Œæ˜¯å‡è®¾é‡‡å–çš„æ˜¯è®©next_stateä¸‹q-tableå€¼æœ€å¤§çš„åŠ¨ä½œã€‚&lt;/p&gt;"/>
    <script type="application/ld+json">
        {
            "description": "&lt;h2 id=&#34;Q-Learning&#34;&gt;&lt;a href=&#34;#Q-Learning&#34; class=&#34;headerlink&#34; title=&#34;Q-Learning&#34;&gt;&lt;/a&gt;Q-Learning&lt;/h2&gt;&lt;h3 id=&#34;åŸç†&#34;&gt;&lt;a href=&#34;#åŸç†&#34; class=&#34;headerlink&#34; title=&#34;åŸç†&#34;&gt;&lt;/a&gt;åŸç†&lt;/h3&gt;&lt;p&gt;è¯¾ç¨‹å‚è€ƒï¼š&lt;a href=&#34;https://www.bilibili.com/video/BV13W411Y75P&#34;&gt;https://www.bilibili.com/video/BV13W411Y75P&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Q-Learningæ˜¯å±äºå€¼å‡½æ•°è¿‘ä¼¼ç®—æ³•ä¸­ï¼Œè’™ç‰¹å¡æ´›æ–¹æ³•å’Œæ—¶é—´å·®åˆ†æ³•ç›¸ç»“åˆçš„ç®—æ³•ã€‚è¿™ç§ç®—æ³•ä½¿å¾—æ™ºèƒ½ä½“ï¼ˆagentï¼‰èƒ½å¤Ÿåœ¨ä¸ç¯å¢ƒäº’åŠ¨çš„è¿‡ç¨‹ä¸­å­¦ä¹ å¦‚ä½•é‡‡å–åŠ¨ä½œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚Q-learningç‰¹åˆ«é€‚ç”¨äºè§£å†³å†³ç­–è¿‡ç¨‹é—®é¢˜ï¼Œå°¤å…¶æ˜¯é‚£äº›çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å®šä¹‰æ˜ç¡®çš„é—®é¢˜ã€‚&lt;/p&gt;
&lt;p&gt;Q-Learning æ˜¯ä¸€ä¸ªç¦»çº¿ç­–ç•¥ï¼ˆoff-policyï¼‰å­¦ä¹ ç®—æ³•ã€‚åœ¨Q-Learningä¸­ï¼Œæ™ºèƒ½ä½“å­¦ä¹ çš„æ˜¯ä¸€ä¸ªä¸å…¶å®é™…æ‰§è¡ŒåŠ¨ä½œæ— å…³çš„ä¼˜åŒ–ç­–ç•¥ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“å®ƒåœ¨æ¢ç´¢æ›´å¤šçš„çŠ¶æ€-åŠ¨ä½œå¯¹æ—¶ï¼Œå®ƒå­¦ä¹ çš„æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚åŒæ—¶ï¼Œåœ¨æ›´æ–°q-tableä¸­çš„å€¼æ—¶ï¼Œå¹¶ä¸è€ƒè™‘ä¸‹ä¸€æ­¥å®é™…æ‰§è¡Œçš„åŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Œè€Œæ˜¯å‡è®¾é‡‡å–çš„æ˜¯è®©next_stateä¸‹q-tableå€¼æœ€å¤§çš„åŠ¨ä½œã€‚&lt;/p&gt;",
            "author": { "@type": "Person", "name": "ios" },
            "@type": "BlogPosting",
            "url": "http://ioo0s.art/2024/04/10/Reinforcement-Learning-Note/index.html",
            "publisher": {
            "@type": "Organization",
            "logo": {
                "@type": "ImageObject",
                "url": "http://ioo0s.art/images/avatar.jpg"
            },
            "name": "ios"
            },
            "headline": "Reinforcement Learning Note | ioo0s&#39;s blog",
            "datePublished": "2024-04-10T05:13:58.000Z",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "http://ioo0s.art/2024/04/10/Reinforcement-Learning-Note/index.html"
            },
            "@context": "http://schema.org"
        }
    </script>




    

    

    

    

    
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸ¦ˆ</text></svg>">
    

    

    
        <link href="https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/artalk/2.8.6/Artalk.css" rel="stylesheet" />
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@latest/build/styles/vs2015.min.css">
    
<link rel="stylesheet" href="/dist/build.css?v=1654266144177.css">


    
<link rel="stylesheet" href="/dist/custom.css?v=1654266144177.css">


    <script>
        window.isPost = true
        window.aomori = {
            
            
            

            
        }
        window.aomori_logo_typed_animated = true
        window.aomori_search_algolia = false

    </script>

<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="ioo0s's blog" type="application/atom+xml">
</head>

<body>

    <div class="container">
    <header class="header">
        <div class="header-type">
            
            <div class="header-type-avatar avatar avatar-sm">
                <img src="/images/avatar.jpg" alt="ios">
            </div>
            
            <div class="header-type-inner">
                
                    <div id="typed-strings" style="display:none">
                        <p>ioo0s&#39;s blog</p>
                    </div>
                    <a class="header-type-title" id="typed" href="/"></a>
                
    
                
            </div>
        </div>
        <div class="header-menu">
            <div class="header-menu-inner">
                
                <a href="/">ä¸»é¡µ</a>
                
                <a href="/archives">å­˜æ¡£</a>
                
                <a href="/collection">æ”¶è—</a>
                
                <a href="/about">å…³äº</a>
                
            </div>
            <div class="header-menu-social">
                
    <a class="social" target="_blank" href="https://github.com/ioo0s">
        <ion-icon name="logo-github"></ion-icon>
    </a>

    <a class="social" target="_blank" href="https://twitter.com/LiuIos">
        <ion-icon name="logo-twitter"></ion-icon>
    </a>

    <a class="social" target="_blank" href="http://ioo0s.art/atom.xml">
        <ion-icon name="logo-rss"></ion-icon>
    </a>

            </div>
        </div>

        <div class="header-menu-mobile">
            <div class="header-menu-mobile-inner" id="mobile-menu-open">
                <i class="icon icon-menu"></i>
            </div>
        </div>
    </header>

    <div class="header-menu-mobile-menu">
        <div class="header-menu-mobile-menu-bg"></div>
        <div class="header-menu-mobile-menu-wrap">
            <div class="header-menu-mobile-menu-inner">
                <div class="header-menu-mobile-menu-close" id="mobile-menu-close">
                    <i class="icon icon-cross"></i>
                </div>
                <div class="header-menu-mobile-menu-list">
                    
                    <a href="/">ä¸»é¡µ</a>
                    
                    <a href="/archives">å­˜æ¡£</a>
                    
                    <a href="/collection">æ”¶è—</a>
                    
                    <a href="/about">å…³äº</a>
                    
                </div>
            </div>
        </div>
    </div>

</div>

    <div class="container">
        <div class="main">
            <section class="inner">
                <section class="inner-main">
                    <div class="post">
    <article id="post-clutuuncs0000ds1wbl5ydu5p" class="article article-type-post" itemscope
    itemprop="blogPost">

    <div class="article-inner">

        
          
        
        
        

        
        <header class="article-header">
            
  
    <h1 class="article-title" itemprop="name">
      Reinforcement Learning Note
    </h1>
  

        </header>
        

        <div class="article-more-info article-more-info-post hairline">

            <div class="article-date">
  <time datetime="2024-04-10T05:13:58.000Z" itemprop="datePublished">2024-04-10</time>
</div>

            
            <div class="article-category">
                <a class="article-category-link" href="/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">åŸºç¡€çŸ¥è¯†</a>
            </div>
            

            
            <div class="article-tag">
                <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RL/" rel="tag">RL</a></li></ul>
            </div>
            

            
            <div class="article-busuanzi">
                <span id="busuanzi_value_page_pv">N</span> äººçœ‹è¿‡
            </div>
            

        </div>

        <div class="article-entry post-inner-html hairline" itemprop="articleBody">
            <h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><h3 id="åŸç†"><a href="#åŸç†" class="headerlink" title="åŸç†"></a>åŸç†</h3><p>è¯¾ç¨‹å‚è€ƒï¼š<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV13W411Y75P">https://www.bilibili.com/video/BV13W411Y75P</a></p>
<p>Q-Learningæ˜¯å±äºå€¼å‡½æ•°è¿‘ä¼¼ç®—æ³•ä¸­ï¼Œè’™ç‰¹å¡æ´›æ–¹æ³•å’Œæ—¶é—´å·®åˆ†æ³•ç›¸ç»“åˆçš„ç®—æ³•ã€‚è¿™ç§ç®—æ³•ä½¿å¾—æ™ºèƒ½ä½“ï¼ˆagentï¼‰èƒ½å¤Ÿåœ¨ä¸ç¯å¢ƒäº’åŠ¨çš„è¿‡ç¨‹ä¸­å­¦ä¹ å¦‚ä½•é‡‡å–åŠ¨ä½œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚Q-learningç‰¹åˆ«é€‚ç”¨äºè§£å†³å†³ç­–è¿‡ç¨‹é—®é¢˜ï¼Œå°¤å…¶æ˜¯é‚£äº›çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å®šä¹‰æ˜ç¡®çš„é—®é¢˜ã€‚</p>
<p>Q-Learning æ˜¯ä¸€ä¸ªç¦»çº¿ç­–ç•¥ï¼ˆoff-policyï¼‰å­¦ä¹ ç®—æ³•ã€‚åœ¨Q-Learningä¸­ï¼Œæ™ºèƒ½ä½“å­¦ä¹ çš„æ˜¯ä¸€ä¸ªä¸å…¶å®é™…æ‰§è¡ŒåŠ¨ä½œæ— å…³çš„ä¼˜åŒ–ç­–ç•¥ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“å®ƒåœ¨æ¢ç´¢æ›´å¤šçš„çŠ¶æ€-åŠ¨ä½œå¯¹æ—¶ï¼Œå®ƒå­¦ä¹ çš„æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚åŒæ—¶ï¼Œåœ¨æ›´æ–°q-tableä¸­çš„å€¼æ—¶ï¼Œå¹¶ä¸è€ƒè™‘ä¸‹ä¸€æ­¥å®é™…æ‰§è¡Œçš„åŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Œè€Œæ˜¯å‡è®¾é‡‡å–çš„æ˜¯è®©next_stateä¸‹q-tableå€¼æœ€å¤§çš„åŠ¨ä½œã€‚</p>
<span id="more"></span>

<h4 id="ç®—æ³•ç‰¹æ€§"><a href="#ç®—æ³•ç‰¹æ€§" class="headerlink" title="ç®—æ³•ç‰¹æ€§"></a>ç®—æ³•ç‰¹æ€§</h4><p><strong>æ— æ¨¡å‹</strong>ï¼šQ-learningæ˜¯ä¸€ä¸ªæ— æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒä¸éœ€è¦å…³äºç¯å¢ƒåŠ¨æ€çš„å…ˆéªŒçŸ¥è¯†</p>
<p><strong>ç¦»çº¿å­¦ä¹ </strong>ï¼šQ-learningæ˜¯ä¸€ç§ç¦»çº¿ç­–ç•¥å­¦ä¹ æ–¹æ³•ï¼Œæ™ºèƒ½ä½“çš„å­¦ä¹ ä¸å…¶éµå¾ªçš„ç­–ç•¥æ— å…³ã€‚</p>
<p><strong>è´ªå©ªç­–ç•¥</strong>ï¼šåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼ŒQ-learningé‡‡ç”¨è´ªå©ªç­–ç•¥åœ¨å­¦ä¹ ä¸æ¢ç´¢é—´å¯»æ‰¾å¹³è¡¡ã€‚å³åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹é€‰æ‹©å½“å‰ä¼°è®¡æœ€ä¼˜çš„åŠ¨ä½œï¼Œä½†æœ‰æ—¶ä¹Ÿä¼šéšæœºé€‰æ‹©å…¶ä»–åŠ¨ä½œæ¥æ¢ç´¢æœªçŸ¥çš„çŠ¶æ€ç©ºé—´ã€‚</p>
<h3 id="è¿·å®«å®ä¾‹"><a href="#è¿·å®«å®ä¾‹" class="headerlink" title="è¿·å®«å®ä¾‹"></a>è¿·å®«å®ä¾‹</h3><h4 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a><strong>Environment</strong></h4><ol>
<li><p>è¿·å®«ç”Ÿæˆ</p>
<p>ç”¨äºéšæœºç”Ÿæˆè¿·å®«ï¼Œæˆ–è€…åŠ è½½ä¸€ä¸ªç”Ÿæˆå¥½çš„è¿·å®«ã€‚è¿·å®«ç”±<code>*</code> <code> </code> æ„æˆï¼Œå…¶ä¸­<code>*</code>ä»£è¡¨å¢™ï¼Œ<code> </code>ä»£è¡¨è·¯ï¼Œ<code>S</code>ä»£è¡¨èµ·ç‚¹ï¼Œ<code>E</code>ä»£è¡¨ç»ˆç‚¹</p>
<pre><code class="python">class Maze:
    def __init__(self, width, height):
        self.width = width
        self.height = height
        self.maze = [[&#39;*&#39; for _ in range(2 * height + 1)] for _ in range(2 * width + 1)]
        self.start_local = None
        self.final_local = None
        self.generate_maze()
        self._locate_start_and_final()

    def _break_wall(self, x, y):
        self.maze[2 * x + 1][2 * y + 1] = &#39; &#39;

    def _carve_passages_from(self, x, y):
        dire = [(1, 0), (-1, 0), (0, 1), (0, -1)]
        random.shuffle(dire)
        for dx, dy in dire:
            new_x = x + dx
            new_y = y + dy
            if 0 &lt;= new_x &lt; self.width and 0 &lt;= new_y &lt; self.height:
                if self.maze[2 * new_x + 1][2 * new_y + 1] == &#39;*&#39;:
                    self.maze[2 * x + 1 + dx][2 * y + 1 + dy] = &#39; &#39;  # Break wall
                    self._break_wall(new_x, new_y)
                    self._carve_passages_from(new_x, new_y)

    def generate_maze(self):
        self._break_wall(0, 0)  # Start point
        self._carve_passages_from(0, 0)
        self.maze[0][1] = &#39;S&#39;  # Mark the start point
        self.maze[2 * self.width][2 * self.height - 1] = &#39;E&#39;  # Mark the end point

    def _locate_start_and_final(self):
        self.start_local = None
        self.final_local = None

        for i, row in enumerate(self.maze):
            for j, char in enumerate(row):
                if char == &#39;S&#39;:
                    self.start_local = (i, j)
                elif char == &#39;E&#39;:
                    self.final_local = (i, j)

        if self.start_local is None or self.final_local is None:
            raise ValueError(&quot;èµ·ç‚¹æˆ–ç»ˆç‚¹æœªåœ¨è¿·å®«ä¸­æ‰¾åˆ°ã€‚&quot;)

    def display(self, maze=None):
        if maze is None:
            for row in self.maze:
                print(&#39;&#39;.join(row))
        else:
            for row in maze:
                print(&#39;&#39;.join(row))

    def save(self, filename):
        with open(filename, &#39;w&#39;) as file:
            for row in self.maze:
                file.write(&#39;&#39;.join(row) + &#39;\n&#39;)

    @classmethod
    def load(cls, filename):
        with open(filename, &#39;r&#39;) as file:
            maze_data = [list(line.strip()) for line in file]
        # å‡è®¾æ–‡ä»¶å†…å®¹ç¡®å®šè¿·å®«å°ºå¯¸ä¸”è¿·å®«è§„æ ¼æ˜¯è§„æ•´çš„ï¼ˆæ¯è¡Œé•¿åº¦ç›¸åŒï¼‰
        height = len(maze_data)
        width = len(maze_data[0]) if height &gt; 0 else 0
        maze_obj = cls(width, height)  # åˆ›å»º Maze å®ä¾‹
        maze_obj.maze = maze_data
        maze_obj._locate_start_and_final()
        return maze_obj
</code></pre>
<p>è¿·å®«å¯ä»¥ä½¿ç”¨saveä¿å­˜åœ¨æœ¬åœ°ï¼Œæ–¹ä¾¿ä¸‹æ¬¡è®­ç»ƒä½¿ç”¨ï¼Œä¿å­˜åçš„å†…å®¹å¦‚ä¸‹ï¼š</p>
<p><img src="/2024/04/10/Reinforcement-Learning-Note/image-20240402104606241.png" alt="image-20240402104606241"></p>
</li>
<li><p>ç§»åŠ¨åˆ¤å®š</p>
<p>ç†è§£ä¸ºæ¸¸æˆçš„æ¨¡æ‹Ÿè¾“å…¥ï¼Œå‡½æ•°çš„è¾“å…¥ä¸ºå½“å‰çš„åæ ‡state(x,y)å’Œæ¥ä¸‹æ¥çš„è¡Œä¸ºaction(u,d,l,f)ã€‚è¾“å‡ºä¸ºæ‰§è¡Œå®Œactionåçš„åæ ‡next_stateï¼Œå’Œå¥–åŠ±rewardï¼ˆç”¨äºåˆ¤å®šæ˜¯å¦è¾¾åˆ°ç»ˆç‚¹ï¼‰ã€‚</p>
<pre><code class="python">    def get_env_feedback(self, state, action):
        &quot;&quot;&quot;
        æ ¹æ®å½“å‰çš„çŠ¶æ€å’Œè¡ŒåŠ¨ï¼Œè¿”å›ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ã€‚
        state: å½“å‰çš„çŠ¶æ€ï¼Œå³å½“å‰çš„åæ ‡ (x, y)
        action: å½“å‰é‡‡å–çš„è¡ŒåŠ¨ã€‚&#39;UP&#39;, &#39;DOWN&#39;, &#39;LEFT&#39;, &#39;RIGHT&#39; ä¸­çš„ä¸€ä¸ªã€‚
        è¿”å›: ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ã€‚
        &quot;&quot;&quot;
        # è®¡ç®—ä¸‹ä¸€æ­¥çš„ä½ç½®
        x, y = state
        if action == &#39;UP&#39;:
            next_state = (max(x - 1, 0), y)
        elif action == &#39;DOWN&#39;:
            next_state = (min(x + 1, 2 * self.height), y)
        elif action == &#39;LEFT&#39;:
            next_state = (x, max(y - 1, 0))
        elif action == &#39;RIGHT&#39;:
            next_state = (x, min(y + 1, 2 * self.width))
        else:
            next_state = state  # æ— æ•ˆçš„è¡ŒåŠ¨

        # æ£€æŸ¥ä¸‹ä¸€æ­¥æ˜¯å¦ä¸ºå¢™(&#39;*&#39;)æˆ–ç»ˆç‚¹(&#39;E&#39;)
        next_x, next_y = next_state
        if self.maze[next_x][next_y] == &#39;*&#39;:
            reward = -1  # å¦‚æœæ’å¢™ï¼Œç»™äºˆè´Ÿå¥–åŠ±
            next_state = state  # çŠ¶æ€ä¸æ”¹å˜
        elif self.maze[next_x][next_y] == &#39;E&#39;:
            reward = 1  # å¦‚æœåˆ°è¾¾ç»ˆç‚¹ï¼Œç»™äºˆæ­£å¥–åŠ±
        else:
            reward = 0  # å¦åˆ™ï¼Œæ²¡æœ‰å¥–åŠ±

        return next_state, reward
</code></pre>
</li>
<li><p>ç´¢å¼•è½¬æ¢</p>
<p>ç”¨äºå°†x,yåæ ‡è½¬æ¢ä¸ºq-tableç´¢å¼•çš„è¾…åŠ©æ–¹æ³•</p>
<pre><code class="python">    def state_to_index(self, state):
        &quot;&quot;&quot;
        å°† (x, y) åæ ‡è½¬æ¢ä¸º q_table çš„ç´¢å¼•ã€‚
        &quot;&quot;&quot;
        x, y = state
        index = x * self.width + y
        return index
</code></pre>
</li>
</ol>
<h4 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a><strong>Agent</strong></h4><p>ä½¿ç”¨q-leaning</p>
<ol>
<li><p>åˆ›å»ºq-leaningè¡¨</p>
<p>å‚æ•°ä¸ºn_statesï¼šè¿·å®«çš„é•¿*å®½ï¼Œactionsï¼š[â€˜LEFTâ€™, â€˜RIGHTâ€™, â€˜UPâ€™, â€˜DOWNâ€™]</p>
<pre><code class="python">def build_q_table(n_states: int, actions: list[str]) -&gt; pd.DataFrame:
    table = pd.DataFrame(
        np.zeros((n_states, len(actions))),
        columns=actions)
    return table
</code></pre>
<p><img src="/2024/04/10/Reinforcement-Learning-Note/image-20240402101717762.png" alt="image-20240402101717762"></p>
</li>
<li><p>è¡ŒåŠ¨å†³ç­–</p>
<p>é¦–å…ˆè·å–å½“å‰ä½ç½®(state_idx)çš„å†³ç­–æ¦‚ç‡ï¼Œä¾‹å¦‚state_idx=0æ—¶ï¼Œstate_actions = [0.0, 0.0, 0.0, 0.0]ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªè¶…å‚EPSILONï¼Œç”¨äºåœ¨è¡ŒåŠ¨å†³ç­–ä¸­åˆ’åˆ†å¤šå°‘æ¦‚ç‡éšæœºé€‰æ‹©ä¸€æ¬¡è¡ŒåŠ¨ã€‚å¦‚æœä¸ä½¿ç”¨éšæœºå†³ç­–åˆ™ä¼šå–å½“å‰state_actionsä¸­æ¦‚ç‡æœ€å¤§çš„ä¸€ä¸ªå†³ç­–ã€‚</p>
<pre><code class="python">def choose_action(state_idx, q_table: pd.DataFrame) -&gt; str:
    # æ ¹æ®å½“å‰stateçŠ¶æ€å’Œq_tableé€‰æ‹©action
    state_actions: np.ndarray = q_table.iloc[state_idx, :]

    # éšæœºé€‰æ‹©çš„æƒ…å†µ1.åˆšå¥½æ˜¯10%çš„éšæœºçŠ¶æ€ 2.åˆå§‹åŒ–çŠ¶æ€

    if np.random.uniform() &gt; (1 - EPSILON) or state_actions.all() == 0:
        action_name = np.random.choice(ACTIONS)
    else:
        action_name = state_actions.idxmax()
    return action_name
</code></pre>
</li>
</ol>
<h4 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h4><p>å½“Agentå’ŒEnvironmentéƒ½å®ç°åï¼Œå¯ä»¥å¼€å§‹ç¼–å†™q-leaningçš„è®­ç»ƒäº†ã€‚</p>
<pre><code class="python">def save_q_table(q_table):
    # è·å–å½“å‰æ—¥æœŸå¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    date_suffix = datetime.now().strftime(&quot;%Y-%m-%d&quot;)
    filename = f&quot;q_table_&#123;date_suffix&#125;.npy&quot;
    np.save(filename, q_table)
    print(f&quot;Q-table saved to &#123;filename&#125;&quot;)
    
def train(maze):
    q_table = build_q_table(maze.width * maze.height, ACTIONS)
    print(q_table)

    for episode in range(STEP):
        step_counter = 0
        is_final = False

        S = maze.start_local
        maze.update_env(maze, S, episode=episode, step_counter=step_counter)
        while not is_final:
            S_INDEX = maze.state_to_index(S)
            A = choose_action(S_INDEX, q_table)
            observation_, reward = maze.get_env_feedback(S, A)

            q_predict = q_table.loc[S_INDEX, A]
            if reward != 1: # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°è¿·å®«ç»ˆç‚¹
                  # æœªåˆ°è¾¾æ—¶ï¼Œè·å–ä¸‹ä¸€ä¸ªåæ ‡çš„indexï¼Œå¹¶ä¸”è®¡ç®—å¯¹åº”çš„q_targetå€¼
                S__INDEX = maze.state_to_index(observation_)
                # LAMBDAä¸ºè¡°å‡è¶…å‚
                q_target = reward + LAMBDA * q_table.iloc[S__INDEX, :].max()
            else:
                # è¾¾åˆ°æ—¶ q_target=1
                q_target = reward
                is_final = True
                        # æ›´æ–°å‚æ•°ï¼ŒALPHAä¸ºleaning-rateè¶…å‚
            q_table.loc[S_INDEX, A] += ALPHA * (q_target - q_predict)  # æ›´æ–°q-table
            S = observation_

            step_counter += 1
              maze.update_env(maze, S, episode=episode, step_counter=step_counter)

    q_table_numpy = q_table.to_numpy()
    # ä¿å­˜q-table
    save_q_table(q_table_numpy)
    return q_table
</code></pre>
<p>é™„ç®—æ³•å›¾ï¼š</p>
<p><img src="/2024/04/10/Reinforcement-Learning-Note/image-20240402103237474.png" alt="image-20240402103237474"></p>
<p>è®­ç»ƒç»“æœæˆªå›¾ï¼š</p>
<p><img src="/2024/04/10/Reinforcement-Learning-Note/image-20240402103831675.png" alt="image-20240402103831675"></p>
<h4 id="Evaluate"><a href="#Evaluate" class="headerlink" title="Evaluate"></a>Evaluate</h4><ol>
<li><p>ç¼–å†™MazeGUIï¼Œä¸ºäº†è®©æµ‹è¯•å…·åƒåŒ–ï¼Œå¹¶ä¸”ä½¿ç”¨movesç»Ÿè®¡æµ‹è¯•æ—¶ä½¿ç”¨çš„æ­¥éª¤</p>
<pre><code>class MazeGUI:
    def __init__(self, maze):
        self.maze = maze
        self.root = tk.Tk()
        self.root.title(&quot;Maze&quot;)
        self.size = 600  # çª—å£å°ºå¯¸
        self.cell_width = self.size // len(maze.maze[0])
        self.cell_height = self.size // len(maze.maze)
        self.canvas = tk.Canvas(self.root, height=self.size, width=self.size, bg=&quot;white&quot;)
        self.canvas.pack()
        self.draw_maze()
        self.player = self.canvas.create_rectangle(0, 0, self.cell_width, self.cell_height, fill=&quot;blue&quot;)  # åˆå§‹åŒ–ç©å®¶ä½ç½®
        self.gui_queue = Queue()
        self.process_queue_updates()
        self.moves = 0  # ç”¨äºæ­¥æ•°ç»Ÿè®¡

        # åˆ›å»ºæ˜¾ç¤ºæ­¥æ•°çš„Labelç»„ä»¶
        self.steps_label = tk.Label(self.root, text=f&quot;Moves: &#123;self.moves&#125;&quot;)
        self.steps_label.pack()

    def draw_maze(self):
        for i, row in enumerate(self.maze.maze):
            for j, cell in enumerate(row):
                x0 = j * self.cell_width
                y0 = i * self.cell_height
                x1 = x0 + self.cell_width
                y1 = y0 + self.cell_height

                if cell == &#39;*&#39;:  # å¢™å£
                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=&quot;black&quot;)
                elif cell == &#39;E&#39;:  # ç»ˆç‚¹
                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=&quot;red&quot;)
                elif cell == &#39;S&#39;:  # èµ·ç‚¹
                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=&quot;green&quot;)
                elif cell == &#39; &#39;:  # ç©ºè·¯
                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=&quot;white&quot;)

    def update_player_position(self, new_position):
        self.moves += 1  # æ­¥æ•°ç»Ÿè®¡
        self.steps_label.config(text=f&quot;Moves: &#123;self.moves&#125;&quot;)

        x, y = new_position
        if x &lt; 0 or y &lt; 0 or x &gt;= self.maze.height or y &gt;= self.maze.width:
            print(&quot;Invalid move: Player cannot move outside the maze.&quot;)
            return  # è¿”å›ï¼Œä¸æ‰§è¡Œç§»åŠ¨

        # æ£€æŸ¥æ–°ä½ç½®æ˜¯å¦æ˜¯å¢™å£
        if self.maze.maze[x][y] == &#39;*&#39;:
            print(&quot;Invalid move: Player cannot move into a wall.&quot;)
        else:
            # æ›´æ–°ç©å®¶åœ¨ç”»å¸ƒä¸Šçš„åæ ‡ä½ç½®
            self.canvas.coords(self.player,
                               y * self.cell_width,  # å·¦ä¸Šè§’xåæ ‡
                               x * self.cell_height,  # å·¦ä¸Šè§’yåæ ‡
                               (y + 1) * self.cell_width,  # å³ä¸‹è§’xåæ ‡
                               (x + 1) * self.cell_height)  # å³ä¸‹è§’yåæ ‡

    def process_queue_updates(self):
        try:
            while not self.gui_queue.empty():
                new_position = self.gui_queue.get_nowait()
                # å‡è®¾ä½ æœ‰ä¸€ä¸ªæ–¹æ³•æ¥å¤„ç†å®é™…çš„æ›´æ–°
                self.update_player_position(new_position)
        except self.gui_queue.Empty:
            pass
        # æ¯éš”100msæ£€æŸ¥é˜Ÿåˆ—æ›´æ–°
        self.root.after(100, self.process_queue_updates)

    def show_steps(self):
        # è¿™ä¸ªæ–¹æ³•è¢«è°ƒç”¨æ—¶ï¼Œä¼šä½œå‡ºè®¡æ•°å¹¶å¼¹çª—æ˜¾ç¤ºç§»åŠ¨æ¬¡æ•°
        messagebox.showinfo(&quot;Steps&quot;, f&quot;Number of moves: &#123;self.moves&#125;&quot;)

    def reset(self):
        # # æ¸…é™¤ç”»å¸ƒä¸Šçš„æ‰€æœ‰å†…å®¹
        # self.canvas.delete(&quot;all&quot;)
        #
        # self.draw_maze()
        # å°†ç©å®¶ç§»åŠ¨åˆ°è¿·å®«çš„èµ·ç‚¹
        self.update_player_position(self.maze.start_local)

    def run(self):
        self.root.mainloop()
</code></pre>
</li>
<li><p>ç¼–å†™evalå‡½æ•°ï¼ŒéªŒè¯æ—¶ä¸éœ€è¦é‡‡ç”¨éšæœºåŒ–å†³ç­–ï¼Œç›´æ¥ä»q-tableä¸­è·å–æ¯ä¸€æ­¥çš„æœ€å¤§å€¼å†³ç­–å³å¯ã€‚</p>
<pre><code>def eval(q_table, maze_gui):
    S = maze_gui.maze.start_local
    is_final = False
    maze_gui.reset()  # é‡ç½®è¿·å®«åˆ°åˆå§‹çŠ¶æ€ï¼Œå¹¶åœ¨GUIä¸­æ›´æ–°

    while not is_final:
        S_INDEX = maze_gui.maze.state_to_index(S)
        # æ€»æ˜¯é€‰æ‹©æœ€ä½³åŠ¨ä½œ
        A = q_table.iloc[S_INDEX, :].idxmax()
        observation_, reward = maze_gui.maze.get_env_feedback(S, A)

        # å¯¹ GUI åšå‡ºæ›´æ–°
        maze_gui.gui_queue.put(observation_)

        # å»¶è¿Ÿä¸€å°æ®µæ—¶é—´ï¼Œä»¥ä¾¿è§‚å¯Ÿåˆ°ç©å®¶ç§»åŠ¨
        time.sleep(0.3)

        S = observation_  # æ›´æ–°å½“å‰çŠ¶æ€

        # ç»ˆç‚¹æ£€æµ‹
        if reward == 1:
            is_final = True

    print(&quot;Evaluation complete.&quot;)
</code></pre>
</li>
</ol>
<h4 id="å®Œæ•´å®ä¾‹"><a href="#å®Œæ•´å®ä¾‹" class="headerlink" title="å®Œæ•´å®ä¾‹"></a>å®Œæ•´å®ä¾‹</h4><p>MazeGen.py</p>
<pre><code class="python">import random
import tkinter as tk
from tkinter import messagebox
from queue import Queue


class MazeGUI:
    def __init__(self, maze):
        self.maze = maze
        self.root = tk.Tk()
        self.root.title(&quot;Maze&quot;)
        self.size = 600  # çª—å£å°ºå¯¸
        self.cell_width = self.size // len(maze.maze[0])
        self.cell_height = self.size // len(maze.maze)
        self.canvas = tk.Canvas(self.root, height=self.size, width=self.size, bg=&quot;white&quot;)
        self.canvas.pack()
        self.draw_maze()
        self.player = self.canvas.create_rectangle(0, 0, self.cell_width, self.cell_height, fill=&quot;blue&quot;)  # åˆå§‹åŒ–ç©å®¶ä½ç½®
        self.gui_queue = Queue()
        self.process_queue_updates()
        self.moves = 0  # ç”¨äºæ­¥æ•°ç»Ÿè®¡

        # åˆ›å»ºæ˜¾ç¤ºæ­¥æ•°çš„Labelç»„ä»¶
        self.steps_label = tk.Label(self.root, text=f&quot;Moves: &#123;self.moves&#125;&quot;)
        self.steps_label.pack()

    def draw_maze(self):
        for i, row in enumerate(self.maze.maze):
            for j, cell in enumerate(row):
                x0 = j * self.cell_width
                y0 = i * self.cell_height
                x1 = x0 + self.cell_width
                y1 = y0 + self.cell_height

                if cell == &#39;*&#39;:  # å¢™å£
                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=&quot;black&quot;)
                elif cell == &#39;E&#39;:  # ç»ˆç‚¹
                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=&quot;red&quot;)
                elif cell == &#39;S&#39;:  # èµ·ç‚¹
                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=&quot;green&quot;)
                elif cell == &#39; &#39;:  # ç©ºè·¯
                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=&quot;white&quot;)

    def update_player_position(self, new_position):
        self.moves += 1  # æ­¥æ•°ç»Ÿè®¡
        self.steps_label.config(text=f&quot;Moves: &#123;self.moves&#125;&quot;)

        x, y = new_position
        if x &lt; 0 or y &lt; 0 or x &gt;= self.maze.height or y &gt;= self.maze.width:
            print(&quot;Invalid move: Player cannot move outside the maze.&quot;)
            return  # è¿”å›ï¼Œä¸æ‰§è¡Œç§»åŠ¨

        # æ£€æŸ¥æ–°ä½ç½®æ˜¯å¦æ˜¯å¢™å£
        if self.maze.maze[x][y] == &#39;*&#39;:
            print(&quot;Invalid move: Player cannot move into a wall.&quot;)
        else:
            # æ›´æ–°ç©å®¶åœ¨ç”»å¸ƒä¸Šçš„åæ ‡ä½ç½®
            self.canvas.coords(self.player,
                               y * self.cell_width,  # å·¦ä¸Šè§’xåæ ‡
                               x * self.cell_height,  # å·¦ä¸Šè§’yåæ ‡
                               (y + 1) * self.cell_width,  # å³ä¸‹è§’xåæ ‡
                               (x + 1) * self.cell_height)  # å³ä¸‹è§’yåæ ‡

    def process_queue_updates(self):
        try:
            while not self.gui_queue.empty():
                new_position = self.gui_queue.get_nowait()
                # å‡è®¾ä½ æœ‰ä¸€ä¸ªæ–¹æ³•æ¥å¤„ç†å®é™…çš„æ›´æ–°
                self.update_player_position(new_position)
        except self.gui_queue.Empty:
            pass
        # æ¯éš”100msæ£€æŸ¥é˜Ÿåˆ—æ›´æ–°
        self.root.after(100, self.process_queue_updates)

    def show_steps(self):
        # è¿™ä¸ªæ–¹æ³•è¢«è°ƒç”¨æ—¶ï¼Œä¼šä½œå‡ºè®¡æ•°å¹¶å¼¹çª—æ˜¾ç¤ºç§»åŠ¨æ¬¡æ•°
        messagebox.showinfo(&quot;Steps&quot;, f&quot;Number of moves: &#123;self.moves&#125;&quot;)

    def reset(self):
        # # æ¸…é™¤ç”»å¸ƒä¸Šçš„æ‰€æœ‰å†…å®¹
        # self.canvas.delete(&quot;all&quot;)
        #
        # self.draw_maze()
        # å°†ç©å®¶ç§»åŠ¨åˆ°è¿·å®«çš„èµ·ç‚¹
        self.update_player_position(self.maze.start_local)

    def run(self):
        self.root.mainloop()


class Maze:
    def __init__(self, width, height):
        self.width = width
        self.height = height
        self.maze = [[&#39;*&#39; for _ in range(2 * height + 1)] for _ in range(2 * width + 1)]
        self.start_local = None
        self.final_local = None
        self.generate_maze()
        self._locate_start_and_final()

    def _break_wall(self, x, y):
        self.maze[2 * x + 1][2 * y + 1] = &#39; &#39;

    def _carve_passages_from(self, x, y):
        dire = [(1, 0), (-1, 0), (0, 1), (0, -1)]
        random.shuffle(dire)
        for dx, dy in dire:
            new_x = x + dx
            new_y = y + dy
            if 0 &lt;= new_x &lt; self.width and 0 &lt;= new_y &lt; self.height:
                if self.maze[2 * new_x + 1][2 * new_y + 1] == &#39;*&#39;:
                    self.maze[2 * x + 1 + dx][2 * y + 1 + dy] = &#39; &#39;  # Break wall
                    self._break_wall(new_x, new_y)
                    self._carve_passages_from(new_x, new_y)

    def generate_maze(self):
        self._break_wall(0, 0)  # Start point
        self._carve_passages_from(0, 0)
        self.maze[0][1] = &#39;S&#39;  # Mark the start point
        self.maze[2 * self.width][2 * self.height - 1] = &#39;E&#39;  # Mark the end point

    def _locate_start_and_final(self):
        self.start_local = None
        self.final_local = None

        for i, row in enumerate(self.maze):
            for j, char in enumerate(row):
                if char == &#39;S&#39;:
                    self.start_local = (i, j)
                elif char == &#39;E&#39;:
                    self.final_local = (i, j)

        if self.start_local is None or self.final_local is None:
            raise ValueError(&quot;èµ·ç‚¹æˆ–ç»ˆç‚¹æœªåœ¨è¿·å®«ä¸­æ‰¾åˆ°ã€‚&quot;)

    def display(self, maze=None):
        if maze is None:
            for row in self.maze:
                print(&#39;&#39;.join(row))
        else:
            for row in maze:
                print(&#39;&#39;.join(row))

    def save(self, filename):
        with open(filename, &#39;w&#39;) as file:
            for row in self.maze:
                file.write(&#39;&#39;.join(row) + &#39;\n&#39;)

    @classmethod
    def load(cls, filename):
        with open(filename, &#39;r&#39;) as file:
            maze_data = [list(line.strip()) for line in file]
        # å‡è®¾æ–‡ä»¶å†…å®¹ç¡®å®šè¿·å®«å°ºå¯¸ä¸”è¿·å®«è§„æ ¼æ˜¯è§„æ•´çš„ï¼ˆæ¯è¡Œé•¿åº¦ç›¸åŒï¼‰
        height = len(maze_data)
        width = len(maze_data[0]) if height &gt; 0 else 0
        maze_obj = cls(width, height)  # åˆ›å»º Maze å®ä¾‹
        maze_obj.maze = maze_data
        maze_obj._locate_start_and_final()
        return maze_obj

    def get_env_feedback(self, state, action):
        &quot;&quot;&quot;
        æ ¹æ®å½“å‰çš„çŠ¶æ€å’Œè¡ŒåŠ¨ï¼Œè¿”å›ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ã€‚
        state: å½“å‰çš„çŠ¶æ€ï¼Œå³å½“å‰çš„åæ ‡ (x, y)
        action: å½“å‰é‡‡å–çš„è¡ŒåŠ¨ã€‚&#39;UP&#39;, &#39;DOWN&#39;, &#39;LEFT&#39;, &#39;RIGHT&#39; ä¸­çš„ä¸€ä¸ªã€‚
        è¿”å›: ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ã€‚
        &quot;&quot;&quot;
        # è®¡ç®—ä¸‹ä¸€æ­¥çš„ä½ç½®
        x, y = state
        if action == &#39;UP&#39;:
            next_state = (max(x - 1, 0), y)
        elif action == &#39;DOWN&#39;:
            next_state = (min(x + 1, 2 * self.height), y)
        elif action == &#39;LEFT&#39;:
            next_state = (x, max(y - 1, 0))
        elif action == &#39;RIGHT&#39;:
            next_state = (x, min(y + 1, 2 * self.width))
        else:
            next_state = state  # æ— æ•ˆçš„è¡ŒåŠ¨

        # æ£€æŸ¥ä¸‹ä¸€æ­¥æ˜¯å¦ä¸ºå¢™(&#39;*&#39;)æˆ–ç»ˆç‚¹(&#39;E&#39;)
        next_x, next_y = next_state
        if self.maze[next_x][next_y] == &#39;*&#39;:
            reward = -1  # å¦‚æœæ’å¢™ï¼Œç»™äºˆè´Ÿå¥–åŠ±
            next_state = state  # çŠ¶æ€ä¸æ”¹å˜
        elif self.maze[next_x][next_y] == &#39;E&#39;:
            reward = 1  # å¦‚æœåˆ°è¾¾ç»ˆç‚¹ï¼Œç»™äºˆæ­£å¥–åŠ±
        else:
            reward = 0  # å¦åˆ™ï¼Œæ²¡æœ‰å¥–åŠ±

        return next_state, reward

    def update_env(self, maze, state, episode, step_counter):
        if state == maze.final_local:
            # å…ˆåˆ›å»ºä¸€ä¸ªè¿·å®«çš„å‰¯æœ¬ä»¥ä¾¿æ›´æ–°æ˜¾ç¤º
            updated_maze = [row.copy() for row in maze.maze]

            # ç¡®å®šç©å®¶çš„å½“å‰ä½ç½®ï¼Œå¹¶æ ‡è®°ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ &#39;P&#39; æ¥è¡¨ç¤ºç©å®¶çš„å½“å‰ä½ç½®
            x, y = state  # å‡è®¾çŠ¶æ€ä¸º (x, y) åæ ‡çš„å‡½æ•°
            updated_maze[x][y] = &#39;P&#39;  # &#39;P&#39; è¡¨ç¤ºç©å®¶å½“å‰ä½ç½®

            # æ¸…å±æ“ä½œï¼Œä»¥ä¾¿æ›´æ–°æ—¶æ¸…é™¤æ—§çš„è¿·å®«çŠ¶æ€
            print(&quot;\033[H\033[J&quot;, end=&quot;&quot;)

            print(f&quot;Episode: &#123;episode&#125;, Step: &#123;step_counter&#125;&quot;)
            self.display(updated_maze)  # å‡è®¾ print_maze æ˜¯æ‰“å°è¿·å®«çŠ¶æ€çš„å‡½æ•°

    def state_to_index(self, state):
        &quot;&quot;&quot;
        å°† (x, y) åæ ‡è½¬æ¢ä¸º q_table çš„ç´¢å¼•ã€‚
        &quot;&quot;&quot;
        x, y = state
        index = x * self.width + y
        return index
</code></pre>
<p>Q-leaning.py</p>
<pre><code class="python">import random
import time
from datetime import datetime
import threading
import numpy as np
import pandas as pd
from MazeGen import MazeGUI, Maze

# è¶…å‚
ACTIONS = [&#39;LEFT&#39;, &#39;RIGHT&#39;, &#39;UP&#39;, &#39;DOWN&#39;]
EPSILON = 0.1  # è´ªå©ªç­–ç•¥ï¼Œå†³ç­–æ¦‚ç‡ï¼ˆ0.1éƒ¨åˆ†ä¸ºéšæœºï¼‰
ALPHA = 0.1  # learning rate
LAMBDA = 0.9  # è¡°å‡å€¼ï¼š 0å®Œå…¨ä¸çœ‹æœªæ¥çš„è§è¿‡ï¼Œ1è€ƒè™‘æœªæ¥çš„æ¯ä¸€ä¸ªç»“æœ
STEP = 300  # è®­ç»ƒè½®æ•°
FRESH_TIME = 0.3  # æ¯ä¸€æ­¥éª¤åœé¡¿æ—¶é—´
random.seed(13)




def build_q_table(n_states: int, actions: list[str]) -&gt; pd.DataFrame:
    table = pd.DataFrame(
        np.zeros((n_states, len(actions))),
        columns=actions)
    return table


def choose_action(state_idx, q_table: pd.DataFrame) -&gt; str:
    # æ ¹æ®å½“å‰stateçŠ¶æ€å’Œq_tableé€‰æ‹©action
    state_actions: np.ndarray = q_table.iloc[state_idx, :]

    # éšæœºé€‰æ‹©çš„æƒ…å†µ1.åˆšå¥½æ˜¯10%çš„éšæœºçŠ¶æ€ 2.åˆå§‹åŒ–çŠ¶æ€
    if np.random.uniform() &gt; EPSILON or state_actions.all() == 0:
        action_name = np.random.choice(ACTIONS)
    else:
        action_name = state_actions.idxmax()
    return action_name


def save_q_table(q_table):
    # è·å–å½“å‰æ—¥æœŸå¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    date_suffix = datetime.now().strftime(&quot;%Y-%m-%d&quot;)
    filename = f&quot;q_table_&#123;date_suffix&#125;.npy&quot;
    np.save(filename, q_table)
    print(f&quot;Q-table saved to &#123;filename&#125;&quot;)


def train(maze):
    q_table = build_q_table(maze.width * maze.height, ACTIONS)
    print(q_table)

    for episode in range(STEP):
        step_counter = 0
        is_final = False

        S = maze.start_local
        maze.update_env(maze, S, episode=episode, step_counter=step_counter)
        while not is_final:
            S_INDEX = maze.state_to_index(S)
            A = choose_action(S_INDEX, q_table)
            observation_, reward = maze.get_env_feedback(S, A)

            q_predict = q_table.loc[S_INDEX, A]
            if reward != 1:
                S__INDEX = maze.state_to_index(observation_)
                q_target = reward + LAMBDA * q_table.iloc[S__INDEX, :].max()
            else:
                q_target = reward
                is_final = True

            q_table.loc[S_INDEX, A] += ALPHA * (q_target - q_predict)  # æ›´æ–°q-table
            S = observation_

            step_counter += 1
            maze.update_env(maze, S, episode=episode, step_counter=step_counter)

    q_table_numpy = q_table.to_numpy()
    # ä¿å­˜q-table
    save_q_table(q_table_numpy)
    return q_table


def eval(q_table, maze_gui):
    S = maze_gui.maze.start_local
    is_final = False
    maze_gui.reset()  # é‡ç½®è¿·å®«åˆ°åˆå§‹çŠ¶æ€ï¼Œå¹¶åœ¨GUIä¸­æ›´æ–°

    while not is_final:
        S_INDEX = maze_gui.maze.state_to_index(S)
        # æ€»æ˜¯é€‰æ‹©æœ€ä½³åŠ¨ä½œ
        A = q_table.iloc[S_INDEX, :].idxmax()
        observation_, reward = maze_gui.maze.get_env_feedback(S, A)

        # å¯¹ GUI åšå‡ºæ›´æ–°
        maze_gui.gui_queue.put(observation_)

        # å»¶è¿Ÿä¸€å°æ®µæ—¶é—´ï¼Œä»¥ä¾¿è§‚å¯Ÿåˆ°ç©å®¶ç§»åŠ¨
        time.sleep(0.3)

        S = observation_  # æ›´æ–°å½“å‰çŠ¶æ€

        # ç»ˆç‚¹æ£€æµ‹
        if reward == 1:
            is_final = True

    print(&quot;Evaluation complete.&quot;)


if __name__ == &#39;__main__&#39;:
    maze = Maze.load(&#39;my_maze.txt&#39;)
    q_table = train(maze)
    print(q_table)
    maze_gui = MazeGUI(maze)

    threading.Thread(target=lambda: eval(q_table, maze_gui)).start()
    maze_gui.root.mainloop()

    # åˆ›å»ºå¹¶æ˜¾ç¤ºè¿·å®«å®ä¾‹
    # my_maze = Maze(4,4)
    # print(&quot;Generated Maze:&quot;)
    # # my_maze.display()
    # #
    # # # ä¿å­˜è¿·å®«åˆ°æ–‡ä»¶
    # my_maze.save(&#39;my_maze.txt&#39;)
    #
    # # # ä»æ–‡ä»¶åŠ è½½å¹¶æ˜¾ç¤ºè¿·å®«
    # loaded_maze = Maze.load(&#39;my_maze.txt&#39;)
    # app = MazeGUI(loaded_maze)
    # app.run()  # æ˜¾ç¤ºè¿·å®«
    # print(&quot;\nLoaded Maze:&quot;)
    # loaded_maze.display()
</code></pre>
<p>æœ€ç»ˆæ•ˆæœå±•ç¤ºï¼š</p>
<p><img src="/2024/04/10/Reinforcement-Learning-Note/image-20240402104515416.png" alt="image-20240402104515416"></p>
<h4 id="å­˜åœ¨é—®é¢˜"><a href="#å­˜åœ¨é—®é¢˜" class="headerlink" title="å­˜åœ¨é—®é¢˜"></a>å­˜åœ¨é—®é¢˜</h4><ol>
<li>q-tableåˆ›å»ºæ²¡æœ‰ä½¿ç”¨åŠ¨æ€åˆ›å»ºï¼Œè¿™ä¼šå¯¼è‡´q-tableçš„indexä¸è¶³æˆ–è€…æµªè´¹çš„æƒ…å†µå‡ºç°ã€‚</li>
<li>chooce actionä¸­idxmaxåªè¿”å›åœ¨è¯·æ±‚è½´ä¸Šç¬¬ä¸€æ¬¡å‡ºç°æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œè¿™å›å¿½ç•¥å½“å‡ºç°æ¯ç§å†³ç­–ç›¸åŒæ¦‚ç‡æ—¶ åªä¼šé€‰æ‹©ç¬¬ä¸€ä¸ªçš„é—®é¢˜ã€‚</li>
</ol>
<h4 id="è§£å†³æ–¹æ¡ˆ"><a href="#è§£å†³æ–¹æ¡ˆ" class="headerlink" title="è§£å†³æ–¹æ¡ˆ"></a>è§£å†³æ–¹æ¡ˆ</h4><ol>
<li>å¯¹QLearningå•ç‹¬å»ºç«‹ç±»ï¼Œå¹¶ä¸”åˆå§‹åŒ–q_tableå†…å®¹ä¸ºç©ºã€‚åˆ©ç”¨check_state_existæ£€æŸ¥å½“å‰statesç´¢å¼•ä»¥åŠä¹‹å‰çš„ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æ–°å»ºã€‚</li>
<li>ä½¿ç”¨state_actions.sample(frac=1)æ¥æ‰“ä¹±actionæ‰€åœ¨ä½ç½®ï¼Œ<a target="_blank" rel="noopener" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html">sample</a>å‡½æ•°ç”¨äºéšæœºæ ·æœ¬è·å–ã€‚</li>
<li>ä¿®æ”¹save_q_tableæ—¶ï¼Œä¾èµ–å½“å‰è·¯å¾„çš„æ€»æ­¥æ•°ï¼Œä¿å­˜æœ€ä¼˜è§£</li>
</ol>
<pre><code>class QLearning:
    def __init__(self, actions: list[str], learning_rate=0.1, reward_decay=0.9, epsilon=0.1):
        self.actions = actions  # åŠ¨ä½œç©ºé—´
        self.lr = learning_rate  # å­¦ä¹ ç‡
        self.gamma = reward_decay  # å¥–åŠ±è¡°å‡
        self.epsilon = epsilon  # æ¢ç´¢æ¦‚ç‡
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)  # åˆå§‹åŒ–ç©ºçš„Qè¡¨

        self.min_steps = float(&#39;inf&#39;)  # åˆå§‹åŒ–æœ€å°‘æ­¥æ•°ä¸ºæ— ç©·å¤§
        self.best_q_table = None  # å­˜å‚¨æ­¥æ•°æœ€å°‘æ—¶çš„Qè¡¨

    def check_state_exist(self, state):
        # æ£€æŸ¥å¹¶æ·»åŠ çŠ¶æ€åˆ°Qè¡¨ï¼ŒåŒ…æ‹¬ä¹‹å‰çš„æ‰€æœ‰æœªæ·»åŠ çš„çŠ¶æ€
        if state not in self.q_table.index:
            # å‡è®¾çŠ¶æ€æ˜¯æ•´æ•°ä¸”è¿ç»­ï¼Œæˆ‘ä»¬éœ€è¦å¡«è¡¥æ‰€æœ‰ç¼ºå¤±çš„çŠ¶æ€ï¼Œç›´è‡³å½“å‰çŠ¶æ€
            missing_states = [s for s in
                              range(min(self.q_table.index.astype(int).min(), state) if not self.q_table.empty else 0,
                                    state + 1) if s not in self.q_table.index]
            for s in missing_states:
                # æ·»åŠ ç¼ºå¤±çš„çŠ¶æ€åˆ°Qè¡¨
                self.q_table = self.q_table._append(
                    pd.Series(
                        [0] * len(self.actions),
                        index=self.q_table.columns,
                        name=s,
                    )
                )

    def choose_action(self, state):
        self.check_state_exist(state)  # ç¡®ä¿çŠ¶æ€åœ¨Qè¡¨ä¸­

        # æ ¹æ®å½“å‰çŠ¶æ€æ¥é€‰æ‹©åŠ¨ä½œ
        state_actions: np.ndarray = self.q_table.iloc[state, :]
        if np.random.uniform() &lt; self.epsilon or state_actions.all() == 0:
            # æ¢ç´¢ï¼šä»¥Îµçš„æ¦‚ç‡æ‰§è¡ŒéšæœºåŠ¨ä½œ
            action = np.random.choice(self.actions)
        else:
            # åˆ©ç”¨ï¼šä»¥1 - Îµçš„æ¦‚ç‡æ‰§è¡Œå½“å‰æœ€ä¼˜åŠ¨ä½œï¼ˆè´ªå©ªé€‰æ‹©ï¼‰
            shuffled_actions = state_actions.sample(frac=1)  # ä½¿ç”¨sampleä¸frac=1æ¥éšæœºæ‰“ä¹±
            action = shuffled_actions.idxmax()
        return action

    def save_q_table(self, steps):
        if steps &lt; self.min_steps:
            self.min_steps = steps
            self.best_q_table = self.q_table.copy()  # æ›´æ–°æœ€ä½³Qè¡¨å‰¯æœ¬

            date_suffix = datetime.now().strftime(&quot;%Y-%m-%d&quot;)
            filename = f&quot;q_learning_q_table_&#123;date_suffix&#125;.npy&quot;
            np.save(filename, self.best_q_table)
            print(f&quot;Q-table saved to &#123;filename&#125;&quot;)

    def learn(self, s, a, r, s_):
        # å­¦ä¹ è¿‡ç¨‹ï¼Œæ ¹æ®q-learningå…¬å¼æ›´æ–°Qè¡¨
        q_predict = self.q_table.loc[s, a]

        if r != 1:
            s__idx = maze.state_to_index(s_)
            self.check_state_exist(s__idx)  # ç¡®ä¿next_statesåœ¨Qè¡¨ä¸­
            q_target = r + self.gamma * self.q_table.iloc[s__idx, :].max()
        else:
            q_target = r

        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # æ›´æ–°q-table
</code></pre>
<p>æ›´æ–°åçš„trainï¼Œå®æ—¶ä¿å­˜æœ€ä¼˜è§£</p>
<pre><code>def train(maze):
    q_learning = QLearning(ACTIONS, learning_rate=ALPHA, reward_decay=LAMBDA, epsilon=EPSILON)

    for episode in range(STEP):
        step_counter = 0

        S = maze.start_local
        maze.update_env(maze, S, episode=episode, step_counter=step_counter)
        while True:
            S_INDEX = maze.state_to_index(S)
            A = q_learning.choose_action(S_INDEX)

            observation_, reward = maze.get_env_feedback(S, A)

            q_learning.learn(S_INDEX, A, reward, observation_)

            S = observation_
            step_counter += 1
            maze.update_env(maze, S, episode=episode, step_counter=step_counter)

            if reward == 1:
                break

        q_learning.save_q_table(step_counter)
    print(&quot;beat steps: &#123;&#125;&quot;.format(q_learning.min_steps))
    return q_learning.best_q_table
</code></pre>
<h2 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h2><h3 id="åŸç†-1"><a href="#åŸç†-1" class="headerlink" title="åŸç†"></a>åŸç†</h3><p>å‚è€ƒè§†é¢‘ï¼š<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV13W411Y75P">https://www.bilibili.com/video/BV13W411Y75P</a></p>
<p>ä¸Q-Learningä¸åŒï¼ŒSARSA æ˜¯ä¸€ä¸ªåœ¨çº¿ç­–ç•¥ï¼ˆon-policyï¼‰å­¦ä¹ ç®—æ³•ã€‚è¿™æ„å‘³ç€å®ƒåœ¨æ›´æ–°å€¼å‡½æ•°æ—¶è€ƒè™‘äº†å½“å‰ç­–ç•¥ä¸‹æ™ºèƒ½ä½“å®é™…ä¼šæ‰§è¡Œçš„åŠ¨ä½œã€‚</p>
<h4 id="ç®—æ³•ç‰¹ç‚¹"><a href="#ç®—æ³•ç‰¹ç‚¹" class="headerlink" title="ç®—æ³•ç‰¹ç‚¹"></a>ç®—æ³•ç‰¹ç‚¹</h4><p><strong>åœ¨çº¿ç­–ç•¥ï¼ˆOn-policyï¼‰</strong>ï¼šSARSAè¯„ä¼°å’Œæ”¹è¿›çš„æ˜¯åŒä¸€ç­–ç•¥ï¼Œå³æ™ºèƒ½ä½“åœ¨å­¦ä¹ æ—¶å®é™…éµå¾ªçš„ç­–ç•¥ã€‚</p>
<p><strong>æ¢ç´¢ä¸åˆ©ç”¨</strong>ï¼šé€šè¿‡ Îµ-è´ªå©ªç­–ç•¥æˆ–å…¶ä»–ç­–ç•¥å¯ä»¥å¹³è¡¡æ¢ç´¢ï¼ˆexplorationï¼‰æ–°çŠ¶æ€-åŠ¨ä½œå¯¹å’Œåˆ©ç”¨ï¼ˆexploitationï¼‰å·²çŸ¥çš„æœ€ä½³çŠ¶æ€-åŠ¨ä½œå¯¹ã€‚</p>
<p><strong>æ”¶æ•›æ€§</strong>ï¼šåœ¨é€‚å½“çš„æ¡ä»¶ä¸‹ï¼ˆå¦‚è¶³å¤Ÿé•¿æ—¶é—´çš„è®­ç»ƒå’Œé€‚å½“çš„è¡°å‡å­¦ä¹ ç‡ï¼‰ï¼ŒSARSAç®—æ³•å¯ä»¥æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚</p>
<h4 id="ä¸Q-Learningä¸»è¦åŒºåˆ«"><a href="#ä¸Q-Learningä¸»è¦åŒºåˆ«" class="headerlink" title="ä¸Q-Learningä¸»è¦åŒºåˆ«"></a>ä¸Q-Learningä¸»è¦åŒºåˆ«</h4><ul>
<li><strong>ç­–ç•¥ç±»å‹</strong>ï¼šQ-Learning æ˜¯ç¦»çº¿ç­–ç•¥ï¼Œæ„å‘³ç€å®ƒåœ¨å­¦ä¹ æœ€ä¼˜ç­–ç•¥æ—¶æ— éœ€éµå¾ªè¯¥ç­–ç•¥ã€‚ç›¸åï¼ŒSARSA æ˜¯åœ¨çº¿ç­–ç•¥ï¼Œå®ƒå¿…é¡»éµå¾ªå½“å‰çš„ç­–ç•¥è¿›è¡Œå­¦ä¹ ã€‚</li>
<li><strong>é£é™©æ€åº¦</strong>ï¼šç”±äº Q-Learning è€ƒè™‘çš„æ˜¯æœ€ä¼˜åŠ¨ä½œï¼Œå®ƒå¯èƒ½ä¼šè¡¨ç°å¾—æ›´åŠ ç§¯æï¼ˆé£é™©åå¥½ï¼‰ã€‚è€ŒSARSAå°†ä¼šè€ƒè™‘å½“å‰çš„æ¢ç´¢æ°´å¹³ï¼Œå› æ­¤å®ƒåœ¨æ›´æ–°è¿‡ç¨‹ä¸­å¯èƒ½æ›´åŠ ä¿å®ˆï¼ˆé£é™©è§„é¿ï¼‰ã€‚</li>
<li><strong>æ”¶æ•›æ€§</strong>ï¼šä¸¤è€…éƒ½å¯ä»¥åœ¨é€‚å½“çš„æ¡ä»¶ä¸‹æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚ç„¶è€Œï¼Œåœ¨å«æœ‰éšæœºå› ç´ æˆ–æ˜¯åŠ¨ä½œé€‰æ‹©æœ‰å™ªå£°çš„æƒ…å†µä¸‹ï¼Œç”±äºSARSAè¾ƒä¸ºä¿å®ˆï¼Œå®ƒé€šå¸¸ä¼šæ›´ç¨³å¥ä¸€äº›ã€‚</li>
</ul>
<h3 id="è¿·å®«å®ä¾‹-1"><a href="#è¿·å®«å®ä¾‹-1" class="headerlink" title="è¿·å®«å®ä¾‹"></a>è¿·å®«å®ä¾‹</h3><h4 id="Environment-1"><a href="#Environment-1" class="headerlink" title="Environment"></a>Environment</h4><p>ä¸Q-learningå®Œå…¨ä¸€è‡´</p>
<h4 id="Agent-1"><a href="#Agent-1" class="headerlink" title="Agent"></a><strong>Agent</strong></h4><p>åŸºæœ¬ä¸Q-learningä¸€è‡´ï¼Œåªæœ‰learnå‡½æ•°éœ€è¦ä¿®æ”¹ä¸ºsarsaç®—æ³•</p>
<pre><code>class Sarsa:
    def __init__(self, actions: list[str], learning_rate=0.1, reward_decay=0.9, epsilon=0.1):
        self.actions = actions  # åŠ¨ä½œç©ºé—´
        self.lr = learning_rate  # å­¦ä¹ ç‡
        self.gamma = reward_decay  # å¥–åŠ±è¡°å‡
        self.epsilon = epsilon  # æ¢ç´¢æ¦‚ç‡
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)  # åˆå§‹åŒ–ç©ºçš„Qè¡¨

        self.min_steps = float(&#39;inf&#39;)  # åˆå§‹åŒ–æœ€å°‘æ­¥æ•°ä¸ºæ— ç©·å¤§
        self.best_q_table = None  # å­˜å‚¨æ­¥æ•°æœ€å°‘æ—¶çš„Qè¡¨

    def check_state_exist(self, state):
        # æ£€æŸ¥å¹¶æ·»åŠ çŠ¶æ€åˆ°Qè¡¨ï¼ŒåŒ…æ‹¬ä¹‹å‰çš„æ‰€æœ‰æœªæ·»åŠ çš„çŠ¶æ€
        if state not in self.q_table.index:
            # å‡è®¾çŠ¶æ€æ˜¯æ•´æ•°ä¸”è¿ç»­ï¼Œæˆ‘ä»¬éœ€è¦å¡«è¡¥æ‰€æœ‰ç¼ºå¤±çš„çŠ¶æ€ï¼Œç›´è‡³å½“å‰çŠ¶æ€
            missing_states = [s for s in
                              range(min(self.q_table.index.astype(int).min(), state) if not self.q_table.empty else 0,
                                    state + 1) if s not in self.q_table.index]
            for s in missing_states:
                # æ·»åŠ ç¼ºå¤±çš„çŠ¶æ€åˆ°Qè¡¨
                self.q_table = self.q_table._append(
                    pd.Series(
                        [0] * len(self.actions),
                        index=self.q_table.columns,
                        name=s,
                    )
                )

    def choose_action(self, state):
        self.check_state_exist(state)  # ç¡®ä¿çŠ¶æ€åœ¨Qè¡¨ä¸­

        # æ ¹æ®å½“å‰çŠ¶æ€æ¥é€‰æ‹©åŠ¨ä½œ
        state_actions: np.ndarray = self.q_table.iloc[state, :]
        if np.random.uniform() &lt; self.epsilon or state_actions.all() == 0:
            # æ¢ç´¢ï¼šä»¥Îµçš„æ¦‚ç‡æ‰§è¡ŒéšæœºåŠ¨ä½œ
            action = np.random.choice(self.actions)
        else:
            # åˆ©ç”¨ï¼šä»¥1 - Îµçš„æ¦‚ç‡æ‰§è¡Œå½“å‰æœ€ä¼˜åŠ¨ä½œï¼ˆè´ªå©ªé€‰æ‹©ï¼‰
            shuffled_actions = state_actions.sample(frac=1)  # ä½¿ç”¨sampleä¸frac=1æ¥éšæœºæ‰“ä¹±
            action = shuffled_actions.idxmax()
        return action

    def save_q_table(self, steps):
        if steps &lt; self.min_steps:
            self.min_steps = steps
            self.best_q_table = self.q_table.copy(deep=True)  # æ›´æ–°æœ€ä½³Qè¡¨å‰¯æœ¬

            date_suffix = datetime.now().strftime(&quot;%Y-%m-%d&quot;)
            filename = f&quot;sarsa_q_table_&#123;date_suffix&#125;.npy&quot;
            np.save(filename, self.best_q_table)
            print(f&quot;Q-table saved to &#123;filename&#125;&quot;)

    def learn(self, s, a, r, next_s, next_action):
        self.check_state_exist(next_s)  # ç¡®ä¿next_statesåœ¨Qè¡¨ä¸­

        # å­¦ä¹ è¿‡ç¨‹ï¼Œæ ¹æ®q-learningå…¬å¼æ›´æ–°Qè¡¨
        q_predict = self.q_table.loc[s, a]

        if r != 1:
            q_target = r + self.gamma * self.q_table.loc[next_s, next_action]  # åªå¯¹next actionè¿›è¡Œè®¡ç®—
        else:
            q_target = r

        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # æ›´æ–°q-table
</code></pre>
<h4 id="Train-1"><a href="#Train-1" class="headerlink" title="Train"></a>Train</h4><p>éœ€è¦å°†actionè®¡ç®—æ”¾åœ¨åˆå§‹è½®ä¸­ï¼Œå¹¶ä¸”è¿­ä»£ã€‚</p>
<pre><code>def train(maze):
    sarsa = Sarsa(ACTIONS, learning_rate=ALPHA, reward_decay=LAMBDA, epsilon=EPSILON)

    for episode in range(STEP):
        step_counter = 0

        S = maze.start_local
        S_INDEX = maze.state_to_index(S)
        A = sarsa.choose_action(S_INDEX)

        maze.update_env(maze, S, episode=episode, step_counter=step_counter)

        while True:
            observation_, reward = maze.get_env_feedback(S, A)

            next_s_idx = maze.state_to_index(observation_)
            next_action = sarsa.choose_action(next_s_idx)

            sarsa.learn(S_INDEX, A, reward, next_s_idx, next_action)

            S = observation_
            A = next_action

            step_counter += 1
            maze.update_env(maze, S, episode=episode, step_counter=step_counter)

            if reward == 1:
                break

        sarsa.save_q_table(step_counter)

    print(&quot;beat steps: &#123;&#125;&quot;.format(sarsa.min_steps))

    return sarsa.best_q_table
</code></pre>
<p>è®­ç»ƒç»“æœæˆªå›¾(æ³¨æ„ ç”±äºä¿å®ˆçš„ç­–ç•¥ï¼Œéœ€è¦æ›´å¤šè½®è®­ç»ƒæ‰ä¼šå¾—åˆ°æœ€ä¼˜çš„ç»“æœ)ï¼š</p>
<p><img src="/2024/04/10/Reinforcement-Learning-Note/image-20240409085515767.png" alt="image-20240409085515767"></p>
<h4 id="Evaluate-1"><a href="#Evaluate-1" class="headerlink" title="Evaluate"></a>Evaluate</h4><p>ä¸q-learningå®Œå…¨ä¸€è‡´</p>
<h4 id="å®Œæ•´ä»£ç "><a href="#å®Œæ•´ä»£ç " class="headerlink" title="å®Œæ•´ä»£ç "></a>å®Œæ•´ä»£ç </h4><pre><code class="python">import random
import threading
import time
from datetime import datetime

import pandas as pd
import numpy as np
from MazeGen import MazeGUI, Maze

ACTIONS = [&#39;LEFT&#39;, &#39;RIGHT&#39;, &#39;UP&#39;, &#39;DOWN&#39;]
EPSILON = 0.2  # ç­–ç•¥é€‰æ‹©
ALPHA = 0.1  # learning rate
LAMBDA = 0.9  # è¡°å‡å€¼ï¼š 0å®Œå…¨ä¸çœ‹æœªæ¥çš„ç»“æœï¼Œ1è€ƒè™‘æœªæ¥çš„æ¯ä¸€ä¸ªç»“æœ
STEP = 100  # è®­ç»ƒè½®æ•°
FRESH_TIME = 0.3  # æ¯ä¸€æ­¥éª¤åœé¡¿æ—¶é—´
random.seed(13)


class Sarsa:
    def __init__(self, actions: list[str], learning_rate=0.1, reward_decay=0.9, epsilon=0.1):
        self.actions = actions  # åŠ¨ä½œç©ºé—´
        self.lr = learning_rate  # å­¦ä¹ ç‡
        self.gamma = reward_decay  # å¥–åŠ±è¡°å‡
        self.epsilon = epsilon  # æ¢ç´¢æ¦‚ç‡
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)  # åˆå§‹åŒ–ç©ºçš„Qè¡¨

        self.min_steps = float(&#39;inf&#39;)  # åˆå§‹åŒ–æœ€å°‘æ­¥æ•°ä¸ºæ— ç©·å¤§
        self.best_q_table = None  # å­˜å‚¨æ­¥æ•°æœ€å°‘æ—¶çš„Qè¡¨

    def check_state_exist(self, state):
        # æ£€æŸ¥å¹¶æ·»åŠ çŠ¶æ€åˆ°Qè¡¨ï¼ŒåŒ…æ‹¬ä¹‹å‰çš„æ‰€æœ‰æœªæ·»åŠ çš„çŠ¶æ€
        if state not in self.q_table.index:
            # å‡è®¾çŠ¶æ€æ˜¯æ•´æ•°ä¸”è¿ç»­ï¼Œæˆ‘ä»¬éœ€è¦å¡«è¡¥æ‰€æœ‰ç¼ºå¤±çš„çŠ¶æ€ï¼Œç›´è‡³å½“å‰çŠ¶æ€
            missing_states = [s for s in
                              range(min(self.q_table.index.astype(int).min(), state) if not self.q_table.empty else 0,
                                    state + 1) if s not in self.q_table.index]
            for s in missing_states:
                # æ·»åŠ ç¼ºå¤±çš„çŠ¶æ€åˆ°Qè¡¨
                self.q_table = self.q_table._append(
                    pd.Series(
                        [0] * len(self.actions),
                        index=self.q_table.columns,
                        name=s,
                    )
                )

    def choose_action(self, state):
        self.check_state_exist(state)  # ç¡®ä¿çŠ¶æ€åœ¨Qè¡¨ä¸­

        # æ ¹æ®å½“å‰çŠ¶æ€æ¥é€‰æ‹©åŠ¨ä½œ
        state_actions: np.ndarray = self.q_table.iloc[state, :]
        if np.random.uniform() &lt; self.epsilon or state_actions.all() == 0:
            # æ¢ç´¢ï¼šä»¥Îµçš„æ¦‚ç‡æ‰§è¡ŒéšæœºåŠ¨ä½œ
            action = np.random.choice(self.actions)
        else:
            # åˆ©ç”¨ï¼šä»¥1 - Îµçš„æ¦‚ç‡æ‰§è¡Œå½“å‰æœ€ä¼˜åŠ¨ä½œï¼ˆè´ªå©ªé€‰æ‹©ï¼‰
            shuffled_actions = state_actions.sample(frac=1)  # ä½¿ç”¨sampleä¸frac=1æ¥éšæœºæ‰“ä¹±
            action = shuffled_actions.idxmax()
        return action

    def save_q_table(self, steps):
        if steps &lt; self.min_steps:
            self.min_steps = steps
            self.best_q_table = self.q_table.copy(deep=True)  # æ›´æ–°æœ€ä½³Qè¡¨å‰¯æœ¬

            date_suffix = datetime.now().strftime(&quot;%Y-%m-%d&quot;)
            filename = f&quot;sarsa_q_table_&#123;date_suffix&#125;.npy&quot;
            np.save(filename, self.best_q_table)
            print(f&quot;Q-table saved to &#123;filename&#125;&quot;)

    def learn(self, s, a, r, next_s, next_action):
        self.check_state_exist(next_s)  # ç¡®ä¿next_statesåœ¨Qè¡¨ä¸­

        # å­¦ä¹ è¿‡ç¨‹ï¼Œæ ¹æ®q-learningå…¬å¼æ›´æ–°Qè¡¨
        q_predict = self.q_table.loc[s, a]

        if r != 1:
            q_target = r + self.gamma * self.q_table.loc[next_s, next_action]  # åªå¯¹next actionè¿›è¡Œè®¡ç®—
        else:
            q_target = r

        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # æ›´æ–°q-table


def train(maze):
    sarsa = Sarsa(ACTIONS, learning_rate=ALPHA, reward_decay=LAMBDA, epsilon=EPSILON)

    for episode in range(STEP):
        step_counter = 0

        S = maze.start_local
        S_INDEX = maze.state_to_index(S)
        A = sarsa.choose_action(S_INDEX)

        maze.update_env(maze, S, episode=episode, step_counter=step_counter)

        while True:
            observation_, reward = maze.get_env_feedback(S, A)

            next_s_idx = maze.state_to_index(observation_)
            next_action = sarsa.choose_action(next_s_idx)

            sarsa.learn(S_INDEX, A, reward, next_s_idx, next_action)

            S = observation_
            A = next_action

            step_counter += 1
            maze.update_env(maze, S, episode=episode, step_counter=step_counter)

            if reward == 1:
                break

        sarsa.save_q_table(step_counter)

    print(&quot;beat steps: &#123;&#125;&quot;.format(sarsa.min_steps))

    return sarsa.best_q_table


def eval(q_table, maze_gui):
    S = maze_gui.maze.start_local
    is_final = False
    maze_gui.reset()  # é‡ç½®è¿·å®«åˆ°åˆå§‹çŠ¶æ€ï¼Œå¹¶åœ¨GUIä¸­æ›´æ–°

    while not is_final:
        S_INDEX = maze_gui.maze.state_to_index(S)
        # æ€»æ˜¯é€‰æ‹©æœ€ä½³åŠ¨ä½œ
        A = q_table.iloc[S_INDEX, :].idxmax()
        observation_, reward = maze_gui.maze.get_env_feedback(S, A)

        # å¯¹ GUI åšå‡ºæ›´æ–°
        maze_gui.gui_queue.put(observation_)

        # å»¶è¿Ÿä¸€å°æ®µæ—¶é—´ï¼Œä»¥ä¾¿è§‚å¯Ÿåˆ°ç©å®¶ç§»åŠ¨
        time.sleep(0.3)

        S = observation_  # æ›´æ–°å½“å‰çŠ¶æ€

        # ç»ˆç‚¹æ£€æµ‹
        if reward == 1:
            is_final = True

    print(&quot;Evaluation complete.&quot;)


if __name__ == &#39;__main__&#39;:
    maze = Maze.load(&#39;my_maze.txt&#39;)
    q_table = train(maze)
    print(q_table)
    maze_gui = MazeGUI(maze)

    threading.Thread(target=lambda: eval(q_table, maze_gui)).start()
    maze_gui.root.mainloop()
</code></pre>

        </div>

    </div>

    

    

    

    

    

    
<nav class="article-nav">
  
    <a href="/2024/05/03/ATF-FUZZ/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-caption">ä¸‹ä¸€ç¯‡</div>
      <div class="article-nav-title">
        
          ATF-FUZZ
        
      </div>
    </a>
  
  
    <a href="/2024/02/04/%E4%B9%94%E5%A7%86%E6%96%AF%E5%9F%BA%E7%94%9F%E6%88%90%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-caption">ä¸Šä¸€ç¯‡</div>
      <div class="article-nav-title">ä¹”å§†æ–¯åŸºç”Ÿæˆè¯­æ³•åˆ†æç¬”è®°</div>
    </a>
  
</nav>


    <section class="share">
        <div class="share-title">åˆ†äº«</div>
        <a class="share-item" target="_blank"
            href="https://twitter.com/share?text=Reinforcement Learning Note - ioo0s's blog&url=http%3A%2F%2Fioo0s.art%2F2024%2F04%2F10%2FReinforcement-Learning-Note%2F">
            <ion-icon name="logo-twitter"></ion-icon>
        </a>
        <a class="share-item" target="_blank"
            href="https://www.facebook.com/sharer.php?title=Reinforcement Learning Note - ioo0s's blog&u=http%3A%2F%2Fioo0s.art%2F2024%2F04%2F10%2FReinforcement-Learning-Note%2F">
            <ion-icon name="logo-facebook"></ion-icon>
        </a>
        <!-- <a class="share-item" target="_blank"
            href="https://service.weibo.com/share/share.php?title=Reinforcement Learning Note - ioo0s's blog&url=http://ioo0s.art/2024/04/10/Reinforcement-Learning-Note/&pic=">
            <div class="n-icon n-icon-weibo"></div>
        </a> -->
    </section>

</article>






<section class="comments comments-artalk">
    <!-- CSS -->
    <link href="http://123.56.126.46:8080/dist/Artalk.css" rel="stylesheet">

    <!-- JS -->
    <script src="http://123.56.126.46:8080/dist/Artalk.js"></script>

    <div id="comments-artalk"></div>
    <script>
        Artalk.init({
          el:        '#comments-artalk',                // ç»‘å®šå…ƒç´ çš„ Selector
          pageKey:   '',
          pageTitle: '',  // é¡µé¢æ ‡é¢˜ (ç•™ç©ºè‡ªåŠ¨è·å–)
          server:    'http://123.56.126.46:8080',  // åç«¯åœ°å€
          site:      'ioo0s Blog',             // ä½ çš„ç«™ç‚¹å
        })
    </script>
</section>











<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

</div>
                </section>
            </section>

            
            <aside class="sidebar ">
                


<div class="widget" id="widget">
    
      
  <div class="widget-wrap">
    <div class="widget-inner">
      <div class="toc post-toc-html"></div>
    </div>
  </div>

    
      
  <div class="widget-wrap widget-cate">
    <div class="widget-title"><span>Categories</span></div>
    <div class="widget-inner">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">åŸºç¡€çŸ¥è¯†</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%BC%8F%E6%B4%9E%E6%8C%96%E6%8E%98/">æ¼æ´æŒ–æ˜</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%BC%8F%E6%B4%9E%E6%8C%96%E6%8E%98/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">åŸºç¡€çŸ¥è¯†</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/">è®ºæ–‡å­¦ä¹ </a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%80%86%E5%90%91%E5%B7%A5%E7%A8%8B/">é€†å‘å·¥ç¨‹</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap widget-tags">
    <div class="widget-title"><span>Tags</span></div>
    <div class="widget-inner">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ASUS/" rel="tag">ASUS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ATF/" rel="tag">ATF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Acrobat-Reader/" rel="tag">Acrobat Reader</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Adobe/" rel="tag">Adobe</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Apollo8-0/" rel="tag">Apollo8.0</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FUZZ/" rel="tag">FUZZ</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FortiGate/" rel="tag">FortiGate</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Fuzz/" rel="tag">Fuzz</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IOT/" rel="tag">IOT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Juniper/" rel="tag">Juniper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RL/" rel="tag">RL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ios%E9%80%86%E5%90%91/" rel="tag">iosé€†å‘</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%B8%E6%88%8F%E7%A0%B4%E8%A7%A3/" rel="tag">æ¸¸æˆç ´è§£</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BC%8F%E6%B4%9E%E5%A4%8D%E7%8E%B0/" rel="tag">æ¼æ´å¤ç°</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" rel="tag">è‡ªåŠ¨é©¾é©¶</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag">è‡ªç„¶è¯­è¨€å¤„ç†</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap widget-recent-posts">
    <div class="widget-title"><span>Recent Posts</span></div>
    <div class="widget-inner">
      <ul>
        
          <li>
            <a href="/2024/05/03/ATF-FUZZ/">ATF-FUZZ</a>
          </li>
        
          <li>
            <a href="/2024/04/10/Reinforcement-Learning-Note/">Reinforcement Learning Note</a>
          </li>
        
          <li>
            <a href="/2024/02/04/%E4%B9%94%E5%A7%86%E6%96%AF%E5%9F%BA%E7%94%9F%E6%88%90%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0/">ä¹”å§†æ–¯åŸºç”Ÿæˆè¯­æ³•åˆ†æç¬”è®°</a>
          </li>
        
          <li>
            <a href="/2024/01/05/Apollo-8-0%E6%95%99%E7%A8%8B/">Apollo 8.0æ•™ç¨‹</a>
          </li>
        
          <li>
            <a href="/2023/03/15/CVE-2023-21608/">CVE-2023-21608</a>
          </li>
        
      </ul>
    </div>
  </div>

    
      
  <div class="widget-wrap widget-archive">
    <div class="widget-title"><span>Archive</span></div>
    <div class="widget-inner">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/">2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023</a></li></ul>
    </div>
  </div>


    
</div>

<div id="backtop"><i class="icon icon-arrow-up"></i></div>
            </aside>
            
        </div>
    </div>

    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<footer class="footer">
    <div class="footer-wave">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1440 320"><path fill="#3c4859" fill-opacity="1" d="M0,160L60,181.3C120,203,240,245,360,240C480,235,600,181,720,186.7C840,192,960,256,1080,261.3C1200,267,1320,213,1380,186.7L1440,160L1440,320L1380,320C1320,320,1200,320,1080,320C960,320,840,320,720,320C600,320,480,320,360,320C240,320,120,320,60,320L0,320Z"></path></svg>
    </div>


    <!-- Please do not remove this -->
    <!-- å¼€æºä¸æ˜“ï¼Œè¯·å‹¿åˆ é™¤ -->
    <div class="footer-wrap">
        <div class="footer-inner"> 
            ioo0s&#39;s blog &copy; 2024<br>
            Powered By Hexo Â· Theme By Aomori
            <span style="width:66%; display:inline-block"></span>
            <span id="busuanzi_container_site_pv">æœ¬ç«™æ€»è®¿é—®é‡<span id="busuanzi_value_site_pv"></span>æ¬¡</span>
        </div>
       
    </div>

</footer>

<script type="module" src="https://unpkg.com/ionicons@6.0.2/dist/ionicons/ionicons.esm.js"></script>




<!-- <script src="https://unpkg.com/disqusjs@1.3/dist/disqus.js"></script> -->
<script src="https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/artalk/2.8.6/Artalk.js"></script>



<script src="/dist/build.js?1654266144177.js"></script>


<script src="/dist/custom.js?1654266144177.js"></script>



<!-- ç™¾åº¦é“¾æ¥æäº¤ -->
<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        }
        else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>











</body>

</html>